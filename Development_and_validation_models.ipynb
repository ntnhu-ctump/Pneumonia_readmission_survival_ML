{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15571a2",
   "metadata": {},
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd5d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, warnings, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, brier_score_loss,\n",
    "                             roc_curve, precision_recall_curve)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3c43c",
   "metadata": {},
   "source": [
    "## Set some inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = Path(\"dt.csv\")  \n",
    "TEST_PATH  = Path(\"dt.csv\")   \n",
    "RANDOM_STATE = 42\n",
    "CALIB_METHOD = \"isotonic\"  \n",
    "N_FOLDS_MAX  = 5           \n",
    "TARGET_SENS  = 0.70\n",
    "TARGET_SPEC  = 0.70\n",
    "N_BOOT       = 1000       \n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "WEIGHT_CHOICES = [\"WTMEC8YR\",\"WTMEC6YR\",\"WTMEC4YR\",\"WTMEC2YR\"]\n",
    "DESIGN_COLS    = [\"SDMVPSU\",\"SDMVSTRA\"]\n",
    "OPTIONAL_DESIGN= [\"SDDSRVYR\"]\n",
    "ID_COLS        = [\"SEQN\",\"split\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1fc0a3",
   "metadata": {},
   "source": [
    "## Define functions for data processing and model development/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfebb8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_outcome(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"dep01\" in df.columns:\n",
    "        out = df.copy()\n",
    "    elif \"dep\" in df.columns:\n",
    "        y = df[\"dep\"].astype(str).str.lower().map({\"yes\":1,\"no\":0})\n",
    "        out = df.copy(); out[\"dep01\"] = y\n",
    "    else:\n",
    "        raise ValueError(\"Outcome not found: need 'dep01' or 'dep'.\")\n",
    "    out = out[~out[\"dep01\"].isna()].copy()\n",
    "    out[\"dep01\"] = out[\"dep01\"].astype(int)\n",
    "    return out\n",
    "\n",
    "def minimal_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = ensure_outcome(df)\n",
    "    for col in DESIGN_COLS:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    return df.dropna(subset=DESIGN_COLS)\n",
    "\n",
    "def infer_n_cycles(df: pd.DataFrame) -> int:\n",
    "    return int(pd.Series(df[\"SDDSRVYR\"]).nunique()) if \"SDDSRVYR\" in df.columns else 1\n",
    "\n",
    "def get_mec_weights(df: pd.DataFrame) -> np.ndarray:\n",
    "    for c in [\"WTMEC8YR\",\"WTMEC6YR\",\"WTMEC4YR\"]:\n",
    "        if c in df.columns:\n",
    "            w = df[c].to_numpy(dtype=float)\n",
    "            return np.where(np.isfinite(w), w, 0.0)\n",
    "    if \"WTMEC2YR\" in df.columns:\n",
    "        ncyc = max(1, infer_n_cycles(df))\n",
    "        w = (df[\"WTMEC2YR\"] / float(ncyc)).to_numpy(dtype=float)\n",
    "        return np.where(np.isfinite(w), w, 0.0)\n",
    "    raise ValueError(\"No MEC weight found (WTMEC{2,4,6,8}YR).\")\n",
    "\n",
    "def build_groups(df: pd.DataFrame) -> np.ndarray:\n",
    "    if \"SDDSRVYR\" in df.columns:\n",
    "        grp = (df[\"SDDSRVYR\"].astype(str) + \"_\" +\n",
    "               df[\"SDMVSTRA\"].astype(str) + \"_\" +\n",
    "               df[\"SDMVPSU\"].astype(str)).to_numpy()\n",
    "    else:\n",
    "        grp = (df[\"SDMVSTRA\"].astype(str) + \"_\" +\n",
    "               df[\"SDMVPSU\"].astype(str)).to_numpy()\n",
    "    return grp\n",
    "\n",
    "\n",
    "import re\n",
    "EXACT_DROP_RAW = {\"SDMVPSU\",\"SDMVSTRA\",\"SDDSRVYR\",\"SEQN\",\"SPLIT\",\"STRAT\",\"PSU\"}\n",
    "STARTS_WITH = (\"WT\", \"WTS\", \"WTSA\", \"SDMV\", \"SDDS\")     \n",
    "CONTAINS    = (\"STRAT\", \"PSU\")                          \n",
    "\n",
    "def _norm(name: str) -> str:\n",
    "    return str(name).strip().upper()\n",
    "\n",
    "EXACT_DROP = {_norm(x) for x in EXACT_DROP_RAW}\n",
    "\n",
    "def _is_excluded(col: str) -> bool:\n",
    "    c = col.upper()\n",
    "    if c in EXACT_DROP: \n",
    "        return True\n",
    "    if c.startswith(STARTS_WITH): \n",
    "        return True\n",
    "    return any(token in c for token in CONTAINS)\n",
    "\n",
    "def infer_feature_cols(df: pd.DataFrame) -> tuple[list[str], list[str]]:\n",
    "    cols = [c for c in df.columns if c != \"dep01\" and not _is_excluded(c)]\n",
    "    num_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cat_cols = [c for c in cols if c not in num_cols]\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _is_binary_series(s: pd.Series) -> bool:\n",
    "    u = pd.unique(s[~pd.isna(s)])\n",
    "    return len(u) <= 2 and set(pd.Series(u).dropna().astype(float)).issubset({0.0, 1.0})\n",
    "\n",
    "def make_preprocessor(num_cols: list[str], cat_cols: list[str], df_fit: pd.DataFrame) -> ColumnTransformer:\n",
    "    bin_cols = [c for c in num_cols if _is_binary_series(df_fit[c])]\n",
    "    num_nonbin = [c for c in num_cols if c not in bin_cols]\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"sc\",  StandardScaler())\n",
    "    ])\n",
    "    bin_pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\"))  \n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_nonbin),\n",
    "        (\"bin\", bin_pipe, bin_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols)\n",
    "    ], remainder=\"drop\", sparse_threshold=0.0)\n",
    "\n",
    "\n",
    "def feature_names_out(prep: ColumnTransformer) -> List[str]:\n",
    "    try:\n",
    "        return list(prep.get_feature_names_out())\n",
    "    except Exception:\n",
    "        num_names = list(prep.transformers_[0][2])\n",
    "        oh = prep.transformers_[1][1][\"oh\"]\n",
    "        base = list(prep.transformers_[1][2])\n",
    "        cat_names = []\n",
    "        if hasattr(oh, \"categories_\"):\n",
    "            for col, cats in zip(base, oh.categories_):\n",
    "                for c in cats:\n",
    "                    cat_names.append(f\"{col}_{c}\")\n",
    "        else:\n",
    "            cat_names = base\n",
    "        return num_names + cat_names\n",
    "\n",
    "\n",
    "\n",
    "def model_factories():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "    grids = {}\n",
    "\n",
    "    lr = LogisticRegression(penalty=\"elasticnet\", solver=\"saga\",\n",
    "                            max_iter=10000, random_state=RANDOM_STATE)\n",
    "    grids[\"LR\"] = (lr, {\n",
    "        \"classifier__C\": [0.1, 1.0],\n",
    "        \"classifier__l1_ratio\": [0.25, 0.5, 0.75],\n",
    "    })\n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, \n",
    "                                n_jobs=-1, random_state=RANDOM_STATE)\n",
    "    grids[\"RF\"] = (rf, {\n",
    "        \"classifier__n_estimators\": [100, \n",
    "                                     200, 500, \n",
    "                                     800, 1000\n",
    "                                    ],\n",
    "        \"classifier__min_samples_leaf\": [5, 10],\n",
    "        \"classifier__max_depth\": [5 ,10]\n",
    "    })\n",
    "\n",
    "    nb = GaussianNB()\n",
    "    grids[\"NB\"] = (nb, {})  \n",
    "    \n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    grids[\"DT\"] = (dt, {\n",
    "        \"classifier__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"classifier__max_depth\": [None, 5, 8, 12],\n",
    "        \"classifier__min_samples_split\": [2, 5, 10],\n",
    "        \"classifier__min_samples_leaf\": [1, 5, 10],\n",
    "        \"classifier__ccp_alpha\": [0.0, 0.0005, 0.001]\n",
    "                                  })\n",
    "    return grids\n",
    "\n",
    "\n",
    "def metrics_weighted(y, p, w):\n",
    "    auc = roc_auc_score(y, p, sample_weight=w)           \n",
    "    ap  = average_precision_score(y, p, sample_weight=w) \n",
    "    b   = brier_score_loss(y, p, sample_weight=w)        \n",
    "    return auc, ap, b\n",
    "\n",
    "def confusion_at(y, p, w, thr):\n",
    "    yhat = (p >= thr).astype(int)\n",
    "    pos = (y == 1); neg = ~pos\n",
    "    tp = float(np.sum(w[(yhat==1) & pos])); fn = float(np.sum(w[(yhat==0) & pos]))\n",
    "    tn = float(np.sum(w[(yhat==0) & neg])); fp = float(np.sum(w[(yhat==1) & neg]))\n",
    "    sens = tp/(tp+fn) if tp+fn>0 else np.nan\n",
    "    spec = tn/(tn+fp) if tn+fp>0 else np.nan\n",
    "    ppv  = tp/(tp+fp) if tp+fp>0 else np.nan\n",
    "    npv  = tn/(tn+fn) if tn+fn>0 else np.nan\n",
    "    prev = float(np.sum(w[pos]))/float(np.sum(w)) if np.sum(w)>0 else np.nan\n",
    "    return {\"threshold\":float(thr),\"sens\":sens,\"spec\":spec,\"ppv\":ppv,\"npv\":npv,\"prevalence\":prev}\n",
    "\n",
    "def pick_threshold_constraints(y, p, w, sens_min=0.70, spec_min=0.70):\n",
    "    fpr, tpr, thr = roc_curve(y, p, sample_weight=w)\n",
    "    spec = 1.0 - fpr\n",
    "    feasible = (tpr >= sens_min) & (spec >= spec_min)\n",
    "    if np.any(feasible):\n",
    "        idx = np.argmax(tpr * feasible)     \n",
    "        return confusion_at(y, p, w, thr[idx])\n",
    "    mm = np.minimum(tpr, spec)\n",
    "    idx = int(np.argmax(mm))\n",
    "    return confusion_at(y, p, w, thr[idx])\n",
    "\n",
    "def fit_with_weights(pipe, X, y, w):\n",
    "    try:\n",
    "        pipe.fit(X, y, **{\"classifier__sample_weight\": w})\n",
    "    except TypeError:\n",
    "        pipe.fit(X, y)\n",
    "\n",
    "def group_cv_weighted_auc(pipe, params, X, y, w, groups, gkf) -> float:\n",
    "    aucs = []\n",
    "    for tr, va in gkf.split(X, y, groups=groups):\n",
    "        Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "        ytr, yva = y[tr], y[va]\n",
    "        wtr, wva = w[tr], w[va]\n",
    "        pipe.set_params(**params)\n",
    "        fit_with_weights(pipe, Xtr, ytr, wtr)\n",
    "        p = pipe.predict_proba(Xva)[:,1]\n",
    "        aucs.append(roc_auc_score(yva, p, sample_weight=wva))\n",
    "    return float(np.mean(aucs))\n",
    "\n",
    "def oof_predict(pipe, params, X, y, w, groups, gkf):\n",
    "    p = np.zeros_like(y, dtype=float); order = np.zeros_like(y, dtype=int)\n",
    "    fold_id = 0\n",
    "    for tr, va in gkf.split(X, y, groups=groups):\n",
    "        Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "        ytr, yva = y[tr], y[va]\n",
    "        wtr = w[tr]\n",
    "        pipe.set_params(**params)\n",
    "        fit_with_weights(pipe, Xtr, ytr, wtr)\n",
    "        p[va] = pipe.predict_proba(Xva)[:,1]\n",
    "        order[va] = fold_id\n",
    "        fold_id += 1\n",
    "    return p, order\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549719e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e3932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    return minimal_clean(pd.read_csv(path))\n",
    "\n",
    "df_tr = load_dataset(TRAIN_PATH)\n",
    "df_te = load_dataset(TEST_PATH)\n",
    "if df_te.empty:\n",
    "    raise RuntimeError(\"Test set has 0 rows after filtering/split.\")\n",
    "\n",
    "y_tr = df_tr[\"dep01\"].to_numpy(int)\n",
    "y_te = df_te[\"dep01\"].to_numpy(int)\n",
    "\n",
    "w_tr = get_mec_weights(df_tr)\n",
    "w_te = get_mec_weights(df_te)\n",
    "\n",
    "groups_tr = build_groups(df_tr)\n",
    "psu_te    = df_te[\"SDMVPSU\"].astype(str).to_numpy()  \n",
    "strat_te  = df_te[\"SDMVSTRA\"].astype(str).to_numpy()\n",
    "\n",
    "num_cols, cat_cols = infer_feature_cols(df_tr)\n",
    "preproc = make_preprocessor(num_cols, cat_cols, df_tr)\n",
    "\n",
    "X_tr = df_tr[num_cols + cat_cols]\n",
    "X_te = df_te[num_cols + cat_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268b9f1",
   "metadata": {},
   "source": [
    "## Tune and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127bbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "factories = model_factories()\n",
    "n_groups = len(np.unique(groups_tr))\n",
    "if n_groups < 2:\n",
    "    raise ValueError(f\"Grouped CV needs >=2 groups; got {n_groups}.\")\n",
    "n_folds = max(2, min(N_FOLDS_MAX, n_groups))\n",
    "gkf = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "cv_summary = []\n",
    "oof_store  = {}  \n",
    "\n",
    "best_name, best_auc, best_params, best_pipe = None, -np.inf, None, None\n",
    "\n",
    "for name, (est, grid) in factories.items():\n",
    "    pipe = Pipeline([(\"prep\", preproc), (\"classifier\", est)])\n",
    "    if not grid:\n",
    "        auc_cv, params = group_cv_weighted_auc(pipe, {}, X_tr, y_tr, w_tr, groups_tr, gkf), {}\n",
    "    else:\n",
    "        scores = []\n",
    "        keys, vals = zip(*grid.items())\n",
    "        for combo in product(*vals):\n",
    "            params = dict(zip(keys, combo))\n",
    "            aucs = group_cv_weighted_auc(pipe, params, X_tr, y_tr, w_tr, groups_tr, gkf)\n",
    "            scores.append((aucs, params))\n",
    "        auc_cv, params = max(scores, key=lambda t: t[0])\n",
    "    p_oof, fold_id = oof_predict(pipe, params, X_tr, y_tr, w_tr, groups_tr, gkf)\n",
    "    oof_store[name] = p_oof\n",
    "\n",
    "    auc, ap, brier = metrics_weighted(y_tr, p_oof, w_tr)\n",
    "    thr_rule = pick_threshold_constraints(y_tr, p_oof, w_tr, TARGET_SENS, TARGET_SPEC)\n",
    "    row = {\"model\":name, \"cv_auc\":auc_cv, \"oof_auc\":auc, \"oof_prauc\":ap,\n",
    "           \"oof_brier\":brier, \"oof_thr\":thr_rule[\"threshold\"],\n",
    "           \"oof_sens\":thr_rule[\"sens\"], \"oof_spec\":thr_rule[\"spec\"],\n",
    "           \"oof_ppv\":thr_rule[\"ppv\"], \"oof_npv\":thr_rule[\"npv\"]}\n",
    "    cv_summary.append(row)\n",
    "    print(f\"[CV] {name}: mean AUC={auc_cv:.3f} | OOF AUC={auc:.3f} | OOF PR-AUC={ap:.3f} | Brier={brier:.3f}\")\n",
    "\n",
    "    if auc_cv > best_auc:\n",
    "        best_name, best_auc, best_params = name, auc_cv, params\n",
    "        best_pipe = Pipeline([(\"prep\", preproc), (\"classifier\", est)])\n",
    "        if best_params: best_pipe.set_params(**best_params)\n",
    "\n",
    "cv_df = pd.DataFrame(cv_summary).sort_values(\"oof_auc\", ascending=False)\n",
    "print(\"\\n=== TRAIN OOF summary (all tuned models) ===\")\n",
    "print(cv_df.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c8491",
   "metadata": {},
   "source": [
    "## PSU bootstrap for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "from scipy.special import expit, logit\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "EPS = 1e-6  \n",
    "\n",
    "def _clip01(x, eps=EPS):\n",
    "    x = np.asarray(x, float)\n",
    "    return np.clip(x, eps, 1.0 - eps)\n",
    "\n",
    "def ci_percentile(x, lo=2.5, hi=97.5):\n",
    "    x = np.asarray(x, float)\n",
    "    return (float(np.nanpercentile(x, lo)), float(np.nanpercentile(x, hi)))\n",
    "\n",
    "def ci_logit_percentile(x, lo=2.5, hi=97.5):\n",
    "    z = logit(_clip01(x))              \n",
    "    lz, hz = np.nanpercentile(z, [lo, hi])\n",
    "    return (float(expit(lz)), float(expit(hz)))  \n",
    "\n",
    "def metrics_weighted(y, p, w):\n",
    "    return (\n",
    "        roc_auc_score(y, p, sample_weight=w),\n",
    "        average_precision_score(y, p, sample_weight=w),\n",
    "        brier_score_loss(y, p, sample_weight=w),\n",
    "    )\n",
    "\n",
    "def confusion_at(y, p, w, thr):\n",
    "    y = np.asarray(y, int); p = np.asarray(p, float); w = np.asarray(w, float)\n",
    "    yhat = (p >= float(thr)).astype(int)\n",
    "    pos = (y == 1); neg = ~pos\n",
    "    tp = float(np.sum(w[(yhat==1) & pos])); fn = float(np.sum(w[(yhat==0) & pos]))\n",
    "    tn = float(np.sum(w[(yhat==0) & neg])); fp = float(np.sum(w[(yhat==1) & neg]))\n",
    "    sens = tp/(tp+fn) if (tp+fn)>0 else np.nan\n",
    "    spec = tn/(tn+fp) if (tn+fp)>0 else np.nan\n",
    "    ppv  = tp/(tp+fp) if (tp+fp)>0 else np.nan\n",
    "    npv  = tn/(tn+fn) if (tn+fn)>0 else np.nan\n",
    "    return {\"sens\": sens, \"spec\": spec, \"ppv\": ppv, \"npv\": npv}\n",
    "\n",
    "\n",
    "\n",
    "def psu_multiplicity_unstratified(psu_ids, B=1000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    u = np.unique(psu_ids)\n",
    "    for _ in range(B):\n",
    "        draw = rng.choice(u, size=len(u), replace=True)\n",
    "        cnt = Counter(draw)\n",
    "        yield np.vectorize(cnt.get)(psu_ids, 0).astype(float)\n",
    "\n",
    "def psu_multiplicity_within_strata(psu_ids, strata_ids, B=1000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    psu_ids = np.asarray(psu_ids)\n",
    "    strata_ids = np.asarray(strata_ids)\n",
    "    u_strata = np.unique(strata_ids)\n",
    "    n = len(psu_ids)\n",
    "    for _ in range(B):\n",
    "        mult = np.zeros(n, float)\n",
    "        for s in u_strata:\n",
    "            m = (strata_ids == s)\n",
    "            psu_s = np.unique(psu_ids[m])\n",
    "            draw = rng.choice(psu_s, size=len(psu_s), replace=True)\n",
    "            cnt = Counter(draw)\n",
    "            mult[m] = np.vectorize(cnt.get)(psu_ids[m], 0).astype(float)\n",
    "        yield mult\n",
    "\n",
    "def choose_psu_generator(psu_ids, strata_ids=None, B=1000, seed=42):\n",
    "    if strata_ids is None:\n",
    "        print(\"No strata provided: using unstratified PSU bootstrap.\")\n",
    "        return psu_multiplicity_unstratified(psu_ids, B=B, seed=seed)\n",
    "    npsu = pd.Series(psu_ids).groupby(pd.Series(strata_ids)).nunique()\n",
    "    if (npsu >= 2).all():\n",
    "        return psu_multiplicity_within_strata(psu_ids, strata_ids, B=B, seed=seed)\n",
    "    else:\n",
    "        print(\"WARNING: some strata have only 1 PSU → using UNSTRATIFIED PSU bootstrap for CIs.\")\n",
    "        return psu_multiplicity_unstratified(psu_ids, B=B, seed=seed)\n",
    "\n",
    "\n",
    "def bootstrap_metrics_with_counts(\n",
    "    y, p, w, thr, psu_ids, strata_ids=None, B=1000, seed=42,\n",
    "    ci_method=\"logit\"  \n",
    "):\n",
    "    y = np.asarray(y, int); p = np.asarray(p, float); w = np.asarray(w, float)\n",
    "    gen = choose_psu_generator(psu_ids, strata_ids, B=B, seed=seed)\n",
    "\n",
    "    aucs, aps, brs, sens, specs, ppvs, npvs = ([] for _ in range(7))\n",
    "    for mult in gen:\n",
    "        wb = w * mult\n",
    "        if not np.isfinite(wb).any() or wb.sum() == 0:\n",
    "            continue\n",
    "        if (np.sum(wb[y==1]) == 0) or (np.sum(wb[y==0]) == 0):\n",
    "            continue\n",
    "        a  = roc_auc_score(y, p, sample_weight=wb)\n",
    "        ap = average_precision_score(y, p, sample_weight=wb)\n",
    "        b  = brier_score_loss(y, p, sample_weight=wb)\n",
    "        cm = confusion_at(y, p, wb, thr)\n",
    "        aucs.append(a); aps.append(ap); brs.append(b)\n",
    "        sens.append(cm[\"sens\"]); specs.append(cm[\"spec\"])\n",
    "        ppvs.append(cm[\"ppv\"]);  npvs.append(cm[\"npv\"])\n",
    "\n",
    "    if ci_method == \"logit\":\n",
    "        ci_bounded = ci_logit_percentile\n",
    "    elif ci_method == \"percentile\":\n",
    "        ci_bounded = ci_percentile\n",
    "    else:\n",
    "        raise ValueError(\"ci_method must be 'logit' or 'percentile'.\")\n",
    "\n",
    "    return {\n",
    "        \"auc_ci\"  : ci_bounded(aucs),\n",
    "        \"prauc_ci\": ci_bounded(aps),\n",
    "        \"brier_ci\": ci_bounded(brs),\n",
    "        \"sens_ci\" : ci_bounded(sens),\n",
    "        \"spec_ci\" : ci_bounded(specs),\n",
    "        \"ppv_ci\"  : ci_bounded(ppvs),\n",
    "        \"npv_ci\"  : ci_bounded(npvs),\n",
    "        \"n_reps\"  : len(aucs)\n",
    "    }\n",
    "\n",
    "\n",
    "N_BOOT = 1000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "psu_tr  = df_tr[\"SDMVPSU\"].astype(str).to_numpy()\n",
    "stra_tr = df_tr[\"SDMVSTRA\"].astype(str).to_numpy() if \"SDMVSTRA\" in df_tr.columns else None\n",
    "\n",
    "train_ci_rows = []\n",
    "for name, p_oof in oof_store.items():\n",
    "    thr_m = float(cv_df.loc[cv_df[\"model\"]==name, \"oof_thr\"].iloc[0])\n",
    "    auc, ap, brier = metrics_weighted(y_tr, p_oof, w_tr)\n",
    "    cm = confusion_at(y_tr, p_oof, w_tr, thr_m)\n",
    "    ci = bootstrap_metrics_with_counts(\n",
    "        y_tr, p_oof, w_tr, thr_m, psu_tr, strata_ids=stra_tr,\n",
    "        B=N_BOOT, seed=RANDOM_STATE, ci_method=\"logit\"\n",
    "    )\n",
    "    train_ci_rows.append({\n",
    "        \"model\": name,\n",
    "        \"oof_auc\": auc,     \"oof_auc_lo\": ci[\"auc_ci\"][0],   \"oof_auc_hi\": ci[\"auc_ci\"][1],\n",
    "        \"oof_prauc\": ap,    \"oof_prauc_lo\": ci[\"prauc_ci\"][0], \"oof_prauc_hi\": ci[\"prauc_ci\"][1],\n",
    "        \"oof_brier\": brier, \"oof_brier_lo\": ci[\"brier_ci\"][0], \"oof_brier_hi\": ci[\"brier_ci\"][1],\n",
    "        \"oof_thr\": thr_m,\n",
    "        \"oof_sens\": cm[\"sens\"], \"oof_sens_lo\": ci[\"sens_ci\"][0], \"oof_sens_hi\": ci[\"sens_ci\"][1],\n",
    "        \"oof_spec\": cm[\"spec\"], \"oof_spec_lo\": ci[\"spec_ci\"][0], \"oof_spec_hi\": ci[\"spec_ci\"][1],\n",
    "        \"oof_ppv\": cm[\"ppv\"],   \"oof_ppv_lo\": ci[\"ppv_ci\"][0],   \"oof_ppv_hi\": ci[\"ppv_ci\"][1],\n",
    "        \"oof_npv\": cm[\"npv\"],   \"oof_npv_lo\": ci[\"npv_ci\"][0],   \"oof_npv_hi\": ci[\"npv_ci\"][1],\n",
    "        \"oof_boot_reps\": ci[\"n_reps\"]\n",
    "    })\n",
    "\n",
    "train_oof_ci_df = pd.DataFrame(train_ci_rows).sort_values(\"oof_auc\", ascending=False)\n",
    "print(\"\\n=== TRAIN (OOF) PSU-bootstrap 95% CIs — all tuned models (logit-percentile) ===\")\n",
    "print(train_oof_ci_df.round(3)[[\n",
    "    \"model\",\n",
    "    \"oof_auc\",\"oof_auc_lo\",\"oof_auc_hi\",\n",
    "    \"oof_prauc\",\"oof_prauc_lo\",\"oof_prauc_hi\",\n",
    "    \"oof_brier\",\"oof_brier_lo\",\"oof_brier_hi\",\n",
    "    \"oof_sens\",\"oof_sens_lo\",\"oof_sens_hi\",\n",
    "    \"oof_spec\",\"oof_spec_lo\",\"oof_spec_hi\",\n",
    "    \"oof_ppv\",\"oof_ppv_lo\",\"oof_ppv_hi\",\n",
    "    \"oof_npv\",\"oof_npv_lo\",\"oof_npv_hi\",\n",
    "    \"oof_boot_reps\"\n",
    "]])\n",
    "\n",
    "\n",
    "train_oof_ci_df.to_csv(\"train_oof_psu_bootstrap_cis.csv\", index=False)\n",
    "print(\"\\nSaved: train_oof_psu_bootstrap_cis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8e4d9",
   "metadata": {},
   "source": [
    "## Test the best model on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaff99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_with_weights_full(pipe, X, y, w):\n",
    "    try:\n",
    "        pipe.fit(X, y, **{\"classifier__sample_weight\": w})\n",
    "    except TypeError:\n",
    "        pipe.fit(X, y)\n",
    "\n",
    "fit_with_weights_full(best_pipe, X_tr, y_tr, w_tr)\n",
    "p_te = best_pipe.predict_proba(X_te)[:,1]\n",
    "\n",
    "thr_train_best = float(cv_df.loc[cv_df[\"model\"]==best_name, \"oof_thr\"].iloc[0])\n",
    "\n",
    "auc_te, ap_te, brier_te = metrics_weighted(y_te, p_te, w_te)\n",
    "thr_rule_te = confusion_at(y_te, p_te, w_te, thr_train_best)\n",
    "\n",
    "print(f\"\\n=== TEST (best model: {best_name}) ===\")\n",
    "print(f\"ROC-AUC : {auc_te:.3f}\")\n",
    "print(f\"PR-AUC  : {ap_te:.3f}\")\n",
    "print(f\"Brier   : {brier_te:.3f}\")\n",
    "print(f\"Thr({thr_train_best:.4f}) Sens/Spec = {thr_rule_te['sens']:.3f}/{thr_rule_te['spec']:.3f} | \"\n",
    "      f\"PPV/NPV = {thr_rule_te['ppv']:.3f}/{thr_rule_te['npv']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424f1a9",
   "metadata": {},
   "source": [
    "## Recalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae516db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    cal = CalibratedClassifierCV(best_pipe, method=CALIB_METHOD, cv=min(5, n_groups))\n",
    "    try:\n",
    "        cal.fit(X_tr, y_tr, **{\"classifier__sample_weight\": w_tr})\n",
    "    except TypeError:\n",
    "        cal.fit(X_tr, y_tr)\n",
    "    p_te_cal = cal.predict_proba(X_te)[:,1]\n",
    "    auc_cal, ap_cal, brier_cal = metrics_weighted(y_te, p_te_cal, w_te)\n",
    "    thr_rule_te_cal = confusion_at(y_te, p_te_cal, w_te, thr_train_best)  \n",
    "    print(f\"\\n=== TEST after {CALIB_METHOD} calibration ===\")\n",
    "    print(f\"ROC-AUC : {auc_cal:.3f}\")\n",
    "    print(f\"PR-AUC  : {ap_cal:.3f}\")\n",
    "    print(f\"Brier   : {brier_cal:.3f}\")\n",
    "    print(f\"Thr({thr_train_best:.4f}) Sens/Spec = {thr_rule_te_cal['sens']:.3f}/{thr_rule_te_cal['spec']:.3f} | \"\n",
    "          f\"PPV/NPV = {thr_rule_te_cal['ppv']:.3f}/{thr_rule_te_cal['npv']:.3f}\")\n",
    "except Exception as e:\n",
    "    print(\"Calibration skipped:\", e)\n",
    "    p_te_cal = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f2e76",
   "metadata": {},
   "source": [
    "## PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_train_oof_curves(oof_store, y, w, title_prefix=\"TRAIN\"):\n",
    "    plt.figure(figsize=(7,6))\n",
    "    for name, p in oof_store.items():\n",
    "        fpr, tpr, _ = roc_curve(y, p, sample_weight=w)\n",
    "        auc = roc_auc_score(y, p, sample_weight=w)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "    plt.plot([0,1],[0,1],'--',alpha=0.5)\n",
    "    plt.xlabel(\"1 - Specificity\"); plt.ylabel(\"Sensitivity\")\n",
    "    plt.title(f\"{title_prefix} ROC\")\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(\"train_oof_roc.png\", dpi=150)\n",
    "\n",
    "    prev = float(np.sum(w[(y==1)]) / np.sum(w)) if np.sum(w)>0 else 0.0\n",
    "    plt.figure(figsize=(7,6))\n",
    "    for name, p in oof_store.items():\n",
    "        prec, rec, _ = precision_recall_curve(y, p, sample_weight=w)\n",
    "        ap = average_precision_score(y, p, sample_weight=w)\n",
    "        plt.plot(rec, prec, label=f\"{name} (PRAUC={ap:.3f})\")\n",
    "    plt.axhline(prev, linestyle=\"--\", alpha=0.7, label=f\"Baseline (prev={prev:.3f})\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"{title_prefix} PR\")\n",
    "    plt.legend(); plt.tight_layout(); plt.savefig(\"train_oof_pr.png\", dpi=150)\n",
    "\n",
    "\n",
    "def plot_test_best_curves(y, p, w, p_cal=None, title_prefix=\"TEST\"):\n",
    "    plt.figure(figsize=(7,6))\n",
    "    fpr, tpr, _ = roc_curve(y, p, sample_weight=w)\n",
    "    plt.plot(fpr, tpr, label=f\"Best (AUC={roc_auc_score(y,p,sample_weight=w):.3f})\")\n",
    "    if p_cal is not None:\n",
    "        fpr2, tpr2, _ = roc_curve(y, p_cal, sample_weight=w)\n",
    "        plt.plot(fpr2, tpr2, label=f\"Best + calib (AUC={roc_auc_score(y,p_cal,sample_weight=w):.3f})\")\n",
    "    plt.plot([0,1],[0,1],'--',alpha=0.5)\n",
    "    plt.xlabel(\"1 - Specificity\"); plt.ylabel(\"Sensitivity\")\n",
    "    plt.title(f\"{title_prefix} ROC\"); plt.legend(); plt.tight_layout(); plt.savefig(\"test_best_roc.png\", dpi=150)\n",
    "\n",
    "    prev = float(np.sum(w[(y==1)]) / np.sum(w)) if np.sum(w)>0 else 0.0\n",
    "    plt.figure(figsize=(7,6))\n",
    "    pr, rc, _ = precision_recall_curve(y, p, sample_weight=w)\n",
    "    plt.plot(rc, pr, label=f\"Best (AP={average_precision_score(y,p,sample_weight=w):.3f})\")\n",
    "    if p_cal is not None:\n",
    "        pr2, rc2, _ = precision_recall_curve(y, p_cal, sample_weight=w)\n",
    "        plt.plot(rc2, pr2, label=f\"Best + calib (PRAUC={average_precision_score(y,p_cal,sample_weight=w):.3f})\")\n",
    "    plt.axhline(prev, linestyle=\"--\", alpha=0.7, label=f\"Baseline (prev={prev:.3f})\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"{title_prefix} PR\"); plt.legend(); plt.tight_layout(); plt.savefig(\"test_best_pr.png\", dpi=150)\n",
    "\n",
    "plot_train_oof_curves(oof_store, y_tr, w_tr)\n",
    "plot_test_best_curves(y_te, p_te, w_te, p_te_cal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01370ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, average_precision_score\n",
    "\n",
    "def _weighted_quantile(x: np.ndarray, w: np.ndarray, q: np.ndarray) -> np.ndarray:\n",
    "    order = np.argsort(x)\n",
    "    x_sorted, w_sorted = x[order], w[order]\n",
    "    cdf = np.cumsum(w_sorted) / np.sum(w_sorted)\n",
    "    return np.interp(q, cdf, x_sorted)\n",
    "\n",
    "def weighted_calibration_curve(\n",
    "    y: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    w: np.ndarray,\n",
    "    n_bins: int = 10,\n",
    "    strategy: str = \"quantile\",\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    y = np.asarray(y).astype(int)\n",
    "    p = np.asarray(p).astype(float)\n",
    "    w = np.asarray(w).astype(float)\n",
    "    mask = np.isfinite(p) & np.isfinite(w)\n",
    "    y, p, w = y[mask], p[mask], np.clip(w[mask], 0.0, np.inf)\n",
    "\n",
    "\n",
    "    if strategy == \"quantile\":\n",
    "        qs = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        edges = _weighted_quantile(p, w, qs)\n",
    "        edges[0], edges[-1] = 0.0, 1.0\n",
    "    else:\n",
    "        edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "\n",
    "    edges = np.unique(edges)\n",
    "    if len(edges) - 1 < 2:\n",
    "        edges = np.linspace(0.0, 1.0, min(n_bins, 10) + 1)\n",
    "\n",
    "    bin_idx = np.digitize(p, edges[1:-1], right=False)\n",
    "    pred_means, true_means, bin_w = [], [], []\n",
    "    for b in range(len(edges) - 1):\n",
    "        m = bin_idx == b\n",
    "        wb = w[m]\n",
    "        if wb.sum() <= 0:\n",
    "            continue\n",
    "        pb = p[m]\n",
    "        yb = y[m]\n",
    "        pred_means.append(np.average(pb, weights=wb))\n",
    "        true_means.append(np.average(yb, weights=wb))\n",
    "        bin_w.append(wb.sum())\n",
    "\n",
    "    pred_means = np.asarray(pred_means)\n",
    "    true_means = np.asarray(true_means)\n",
    "    bin_w = np.asarray(bin_w)\n",
    "    return pred_means, true_means, edges, bin_w\n",
    "\n",
    "def expected_calibration_error(\n",
    "    y: np.ndarray, p: np.ndarray, w: np.ndarray, n_bins: int = 10\n",
    ") -> float:\n",
    "    pred_b, true_b, _, bin_w = weighted_calibration_curve(y, p, w, n_bins=n_bins, strategy=\"quantile\")\n",
    "    if bin_w.sum() == 0 or len(bin_w) == 0:\n",
    "        return np.nan\n",
    "    return float(np.sum((bin_w / bin_w.sum()) * np.abs(true_b - pred_b)))\n",
    "\n",
    "def _metrics(y, p, w):\n",
    "    return (roc_auc_score(y, p, sample_weight=w),\n",
    "            average_precision_score(y, p, sample_weight=w),\n",
    "            brier_score_loss(y, p, sample_weight=w),\n",
    "            expected_calibration_error(y, p, w, n_bins=10))\n",
    "\n",
    "def plot_calibration_test(y, p, w, p_cal=None, model_name=\"Best\", fname_prefix=\"test_calibration\"):\n",
    "    auc0, ap0, brier0, ece0 = _metrics(y, p, w)\n",
    "    pred0, true0, edges0, bw0 = weighted_calibration_curve(y, p, w, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "    if p_cal is not None:\n",
    "        auc1, ap1, brier1, ece1 = _metrics(y, p_cal, w)\n",
    "        pred1, true1, edges1, bw1 = weighted_calibration_curve(y, p_cal, w, n_bins=10, strategy=\"quantile\")\n",
    "    else:\n",
    "        auc1 = ap1 = brier1 = ece1 = None\n",
    "        pred1 = true1 = None\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.plot([0,1], [0,1], \"--\", lw=1, label=\"Perfectly calibrated\")\n",
    "    plt.plot(pred0, true0, \"o-\", label=f\"{model_name} (AUC={auc0:.3f}, ECE={ece0:.3f}, Brier={brier0:.3f})\")\n",
    "    if pred1 is not None:\n",
    "        plt.plot(pred1, true1, \"o-\", label=f\"{model_name} + calib (AUC={auc1:.3f}, ECE={ece1:.3f}, Brier={brier1:.3f})\")\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Observed positive fraction\")\n",
    "    plt.title(\"Calibration — TEST\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{fname_prefix}_reliability.png\", dpi=150)\n",
    "\n",
    "\n",
    "    bins = np.linspace(0, 1, 21)\n",
    "    plt.figure(figsize=(7,3.5))\n",
    "    plt.hist(p, bins=bins, weights=w, alpha=0.6, label=\"pre-cal\", edgecolor=\"none\")\n",
    "    if p_cal is not None:\n",
    "        plt.hist(p_cal, bins=bins, weights=w, alpha=0.6, label=\"post-cal\", edgecolor=\"none\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.xlabel(\"Predicted probability\")\n",
    "    plt.ylabel(\"Weighted count\")\n",
    "    plt.title(\"Predicted probability distribution — TEST\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{fname_prefix}_hist.png\", dpi=150)\n",
    "\n",
    "    print(\"\\n=== TEST calibration metrics ===\")\n",
    "    print(f\"Pre-cal  : AUC={auc0:.3f} | PR-AUC={ap0:.3f} | Brier={brier0:.3f} | ECE={ece0:.3f}\")\n",
    "    if p_cal is not None:\n",
    "        print(f\"Post-cal : AUC={auc1:.3f} | PR-AUC={ap1:.3f} | Brier={brier1:.3f} | ECE={ece1:.3f}\")\n",
    "    print(f\"Saved: {fname_prefix}_reliability.png, {fname_prefix}_hist.png\")\n",
    "\n",
    "plot_calibration_test(y_te, p_te, w_te, p_cal=(p_te_cal if 'p_te_cal' in globals() else None),\n",
    "                      model_name=best_name, fname_prefix=\"test_calibration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77126a4",
   "metadata": {},
   "source": [
    "## Feature importance ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def model_native_importance(fitted_pipe, prep):\n",
    "    names = list(prep.get_feature_names_out())  \n",
    "    clf = fitted_pipe.named_steps[\"classifier\"]\n",
    "\n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        imp = np.asarray(clf.feature_importances_, dtype=float)\n",
    "        if imp.shape[0] != len(names):\n",
    "            raise ValueError(f\"Length mismatch: {imp.shape[0]} importances vs {len(names)} features.\")\n",
    "        return pd.DataFrame({\"feature\": names, \"importance\": imp, \"source\": \"model\"})\n",
    "\n",
    "    if hasattr(clf, \"coef_\"):\n",
    "        coef = np.asarray(clf.coef_, dtype=float)  \n",
    "        if coef.ndim == 2 and coef.shape[0] > 1:\n",
    "            coef_use = np.mean(np.abs(coef), axis=0)\n",
    "        else:\n",
    "            coef_use = np.abs(coef).ravel()\n",
    "        if coef_use.shape[0] != len(names):\n",
    "            raise ValueError(f\"Length mismatch: {coef_use.shape[0]} coefs vs {len(names)} features.\")\n",
    "        return pd.DataFrame({\"feature\": names, \"importance\": coef_use, \"source\": \"|coef|\"})\n",
    "\n",
    "    return pd.DataFrame(columns=[\"feature\",\"importance\",\"source\"])\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def permutation_importance_weighted(fitted_pipe, X, y, w, n_repeats=10, random_state=42):\n",
    "    base = roc_auc_score(y, fitted_pipe.predict_proba(X)[:, 1], sample_weight=w)\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rows = []\n",
    "    for col in X.columns:\n",
    "        scores = []\n",
    "        for _ in range(n_repeats):\n",
    "            Xp = X.copy()\n",
    "            Xp[col] = Xp[col].sample(frac=1.0, random_state=rng).to_numpy()\n",
    "            p = fitted_pipe.predict_proba(Xp)[:, 1]\n",
    "            s = roc_auc_score(y, p, sample_weight=w)\n",
    "            scores.append(base - s)\n",
    "        rows.append((col, float(np.mean(scores))))\n",
    "    return pd.DataFrame(rows, columns=[\"feature\",\"importance\"]).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "\n",
    "def get_transformed_feature_names(prep):\n",
    "    return list(prep.get_feature_names_out())\n",
    "\n",
    "def build_transformed_to_raw_map(prep, cat_cols):\n",
    "    tnames = get_transformed_feature_names(prep)\n",
    "    mapping = {}\n",
    "\n",
    "    for t in tnames:\n",
    "        if t.startswith(\"num__\"):\n",
    "            mapping[t] = t.split(\"__\", 1)[1]\n",
    "        elif t.startswith(\"bin__\"):\n",
    "            mapping[t] = t.split(\"__\", 1)[1]\n",
    "        elif t.startswith(\"cat__\"):\n",
    "            rest = t.split(\"__\", 1)[1]\n",
    "            base = None\n",
    "            for col in cat_cols:\n",
    "                prefix = f\"{col}_\"\n",
    "                if rest.startswith(prefix):\n",
    "                    base = col\n",
    "                    break\n",
    "            mapping[t] = base if base is not None else rest.split(\"_\", 1)[0]\n",
    "        else:\n",
    "            mapping[t] = t\n",
    "    return mapping\n",
    "\n",
    "prep = best_pipe.named_steps[\"prep\"]          \n",
    "trans2raw = build_transformed_to_raw_map(prep, cat_cols)\n",
    "\n",
    "\n",
    "native_imp = model_native_importance(best_pipe, prep)  \n",
    "if not native_imp.empty:\n",
    "    native_imp = native_imp.rename(columns={\"feature\":\"transformed\"})\n",
    "    native_imp[\"raw_feature\"] = native_imp[\"transformed\"].map(trans2raw)\n",
    "    native_imp[\"source\"] = native_imp.get(\"source\", \"model\")\n",
    "    native_imp = native_imp[[\"raw_feature\",\"importance\",\"source\"]]\n",
    "\n",
    "\n",
    "perm_imp = permutation_importance_weighted(best_pipe, X_te, y_te, w_te, n_repeats=10)  \n",
    "perm_imp = perm_imp.rename(columns={\"feature\":\"raw_feature\"})\n",
    "perm_imp[\"source\"] = \"perm_auc\"\n",
    "\n",
    "\n",
    "feat_combined = pd.concat([native_imp, perm_imp], ignore_index=True)\n",
    "\n",
    "\n",
    "feat_rank_raw = (feat_combined.groupby(\"raw_feature\", as_index=False)[\"importance\"]\n",
    "                 .mean()\n",
    "                 .sort_values(\"importance\", ascending=False))\n",
    "\n",
    "print(\"\\n=== Unified feature ranking (raw columns; no duplicates) — top 20 ===\")\n",
    "print(feat_rank_raw.head(20))\n",
    "feat_rank_raw.to_csv(\"feature_importance_raw.csv\", index=False)\n",
    "\n",
    "\n",
    "def plot_feature_importance(feat_df, top=20, filename=\"feature_importance_top20_raw.png\",\n",
    "                            title=\"Feature importance\"):\n",
    "    topk = (feat_df.sort_values(\"importance\", ascending=False)\n",
    "                    .head(top)\n",
    "                    .iloc[::-1])  \n",
    "    plt.figure(figsize=(8, max(6, int(0.35*len(topk)))))\n",
    "    plt.barh(topk[\"raw_feature\"], topk[\"importance\"])\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150)\n",
    "\n",
    "plot_feature_importance(feat_rank_raw, top=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "psu_te  = df_te[\"SDMVPSU\"].astype(str).to_numpy()\n",
    "stra_te = df_te[\"SDMVSTRA\"].astype(str).to_numpy() if \"SDMVSTRA\" in df_te.columns else None\n",
    "\n",
    "\n",
    "if 'p_te' not in globals():\n",
    "    p_te = best_pipe.predict_proba(X_te)[:, 1]\n",
    "\n",
    "if 'thr_train_best' not in globals():\n",
    "    thr_train_best = float(cv_df.loc[cv_df[\"model\"] == best_name, \"oof_thr\"].iloc[0])\n",
    "\n",
    "\n",
    "ci_test = bootstrap_metrics_with_counts(\n",
    "    y_te, p_te, w_te, thr_train_best, psu_te, strata_ids=stra_te,\n",
    "    B=N_BOOT, seed=RANDOM_STATE, ci_method=\"logit\"\n",
    ")\n",
    "print(\"\\n=== TEST (best) PSU-bootstrap 95% CIs — uncalibrated (logit-percentile) ===\")\n",
    "for k in [\"auc_ci\",\"prauc_ci\",\"brier_ci\",\"sens_ci\",\"spec_ci\",\"ppv_ci\",\"npv_ci\"]:\n",
    "    lo, hi = ci_test[k]\n",
    "    print(f\"{k}: [{lo:.3f}, {hi:.3f}]  (reps used: {ci_test['n_reps']})\")\n",
    "\n",
    "if 'p_te_cal' in globals() and p_te_cal is not None:\n",
    "    ci_test_cal = bootstrap_metrics_with_counts(\n",
    "        y_te, p_te_cal, w_te, thr_train_best, psu_te, strata_ids=stra_te,\n",
    "        B=N_BOOT, seed=RANDOM_STATE, ci_method=\"logit\"\n",
    "    )\n",
    "    print(\"\\n=== TEST (best) PSU-bootstrap 95% CIs — calibrated (logit-percentile) ===\")\n",
    "    for k in [\"auc_ci\",\"prauc_ci\",\"brier_ci\",\"sens_ci\",\"spec_ci\",\"ppv_ci\",\"npv_ci\"]:\n",
    "        lo, hi = ci_test_cal[k]\n",
    "        print(f\"{k}: [{lo:.3f}, {hi:.3f}]  (reps used: {ci_test_cal['n_reps']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.to_csv(\"train_oof_model_summary.csv\", index=False)\n",
    "feat_rank_raw.head(50).to_csv(\"feature_importance_top50.csv\", index=False)\n",
    "print(\"\\nSaved: train_oof_model_summary.csv, feature_importance_top50.csv, \"\n",
    "      \"train_oof_roc.png, train_oof_pr.png, test_best_roc.png, test_best_pr.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7edc682",
   "metadata": {},
   "source": [
    "## Save model to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_te = df_te.reset_index(drop=True)\n",
    "\n",
    "\n",
    "p_out = p_te_cal if 'p_te_cal' in globals() and p_te_cal is not None else p_te\n",
    "\n",
    "w_out = w_te if 'w_te' in globals() else np.ones(len(df_te), dtype=float)\n",
    "\n",
    "preds_A = pd.DataFrame({\n",
    "    \"SEQN\": df_te[\"SEQN\"].astype(int),\n",
    "    \"y\": pd.Series(y_te, index=df_te.index, dtype=int),\n",
    "    \"p\": pd.Series(p_out, index=df_te.index, dtype=float),        \n",
    "    \"w\": pd.Series(w_out, index=df_te.index, dtype=float)\n",
    "})\n",
    "\n",
    "if 'p_te_cal' in globals() and p_te_cal is not None:\n",
    "    preds_A[\"p_uncal\"] = pd.Series(p_te, index=df_te.index, dtype=float)\n",
    "    preds_A[\"p_cal\"]   = pd.Series(p_te_cal, index=df_te.index, dtype=float)\n",
    "\n",
    "for col in [\"SDMVSTRA\",\"SDMVPSU\"]:\n",
    "    if col in df_te.columns:\n",
    "        preds_A[col] = df_te[col].astype(str)   \n",
    "preds_A.to_csv(\"modelA_preds.csv\", index=False)\n",
    "print(\"Saved: modelA_preds.csv\")\n",
    "\n",
    "assert len(preds_A) == len(df_te), \"Row count mismatch\"\n",
    "assert preds_A.notna().all().all(), \"NaNs detected in saved predictions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1390b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
